{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e834bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Standardizing, Renaming, and Gathering Images ---\n",
      "Using Data Directory: /Users/sanadmadani/plant-disease-detection/plant-disease-detection/data_raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Classes: 100%|██████████| 52/52 [05:38<00:00,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collected 57560 images.\n",
      "The unified dataset images are located in: /Users/sanadmadani/plant-disease-detection/plant-disease-detection/jordan_dataset2/images\n",
      "\n",
      "--- Splitting Data and Moving Files ---\n",
      "\n",
      "Processing train split...\n",
      "train: 40292 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving train files: 100%|██████████| 40292/40292 [00:14<00:00, 2800.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing val split...\n",
      "val: 8634 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving val files: 100%|██████████| 8634/8634 [00:02<00:00, 4206.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing test split...\n",
      "test: 8634 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving test files: 100%|██████████| 8634/8634 [00:02<00:00, 3023.44it/s]\n",
      "/Users/sanadmadani/anaconda3/lib/python3.11/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Augmenting Small Classes ---\n",
      "Saving training set class counts...\n",
      "Augmenting Olive___Aculus_olearius_mite, need 23 copies.\n",
      "Augmenting Wheat___healthy, need 298 copies.\n",
      "Augmenting Wheat___Stem_fly, need 380 copies.\n",
      "Augmenting Wheat___Powdery_mildew, need 382 copies.\n",
      "Augmenting Wheat___Black_rust, need 388 copies.\n",
      "Augmenting Wheat___Aphid, need 391 copies.\n",
      "Augmenting Eggplant___Leaf_Spot_Disease, need 392 copies.\n",
      "Augmenting Wheat___Mite, need 392 copies.\n",
      "Augmenting Eggplant___Insect_Pest_Disease, need 396 copies.\n",
      "Augmenting Eggplant___healthy, need 396 copies.\n",
      "Augmenting Eggplant___White_Mold_Disease, need 397 copies.\n",
      "Augmenting Eggplant___Mosaic_Virus_Disease, need 398 copies.\n",
      "Augmenting Eggplant___Small_Leaf_Disease, need 398 copies.\n",
      "Augmenting Cauliflower___healthy, need 400 copies.\n",
      "Augmenting Wheat___Leaf_blight, need 401 copies.\n",
      "Augmenting Eggplant___Wilt_Disease, need 403 copies.\n",
      "Augmenting Cauliflower___Bacterial_spot_rot, need 414 copies.\n",
      "Augmenting Cauliflower___Downy_Mildew, need 417 copies.\n",
      "Augmenting Wheat___Yellow_Rust, need 436 copies.\n",
      "Augmenting Wheat___Brown_leaf_Rust, need 437 copies.\n",
      "Augmenting Wheat___Scab, need 439 copies.\n",
      "Augmenting Cauliflower___Black_Rot, need 448 copies.\n",
      "Augmented images created: 8426\n",
      "\n",
      "✨ Dataset Consolidation and Splitting Complete! ✨\n",
      "The data is ready in the standard PyTorch format:\n",
      "/Users/sanadmadani/plant-disease-detection/plant-disease-detection/jordan_dataset2/images/train/[Crop]/[Disease]/*.jpg\n",
      "/Users/sanadmadani/plant-disease-detection/plant-disease-detection/jordan_dataset2/images/val/[Crop]/[Disease]/*.jpg\n",
      "/Users/sanadmadani/plant-disease-detection/plant-disease-detection/jordan_dataset2/images/test/[Crop]/[Disease]/*.jpg\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil # <--- NEW: Required for moving files\n",
    "from PIL import Image\n",
    "\n",
    "# --- Configuration (Define these based on your setup) ---\n",
    "ROOT = Path(\"/Users/sanadmadani/plant-disease-detection/plant-disease-detection\")\n",
    "DATA_DIR = ROOT / 'data_raw'         # Folder containing 'archive', 'Cauliflower', etc.\n",
    "OUT_DIR = ROOT / 'jordan_dataset2'    # Output folder for metadata and unified 'images'\n",
    "IMAGE_SIZE = 512\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Ensure output paths are defined for the rest of the script\n",
    "IMG_OUT = OUT_DIR / 'images'\n",
    "METADATA_CSV = OUT_DIR / 'metadata_all.csv'\n",
    "# --- End Configuration ---\n",
    "\n",
    "\n",
    "def normalize_and_save(src_path, dest_path, size=IMAGE_SIZE):\n",
    "    \"\"\"Resize, convert to RGB, and save image.\"\"\"\n",
    "    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    try:\n",
    "        img = Image.open(src_path).convert('RGB')\n",
    "        img = img.resize((size, size), Image.Resampling.LANCZOS)\n",
    "        img.save(dest_path, format='JPEG', quality=90)\n",
    "    except Exception as e:\n",
    "        print(f'Failed processing {src_path}: {e}')\n",
    "\n",
    "def file_hash_name(path):\n",
    "    \"\"\"Generate a unique filename based on file content hash.\"\"\"\n",
    "    h = hashlib.sha1()\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                h.update(chunk)\n",
    "        return h.hexdigest() + '.jpg'\n",
    "    except (IOError, OSError) as e:\n",
    "        print(f\"Error hashing file {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Class Mapping (Copied from your input) ---\n",
    "# ... (The CLASS_MAPPING dictionary remains here) ...\n",
    "CLASS_MAPPING = {\n",
    "    # --- From 'archive/train' (Wheat) ---\n",
    "    'Healthy Wheat': ('Wheat', 'healthy'),\n",
    "    'Wheat aphid': ('Wheat', 'Aphid'),\n",
    "    'Wheat black rust': ('Wheat', 'Black_rust'),\n",
    "    'Wheat Brown leaf Rust': ('Wheat', 'Brown_leaf_Rust'),\n",
    "    'Wheat leaf blight': ('Wheat', 'Leaf_blight'),\n",
    "    'Wheat mite': ('Wheat', 'Mite'),\n",
    "    'Wheat powdery mildew': ('Wheat', 'Powdery_mildew'),\n",
    "    'Wheat scab': ('Wheat', 'Scab'),\n",
    "    'Wheat Stem fly': ('Wheat', 'Stem_fly'),\n",
    "    'Wheat___Yellow_Rust': ('Wheat', 'Yellow_Rust'),\n",
    "\n",
    "    # --- From 'Cauliflower/train' (Cauliflower & Eggplant) ---\n",
    "    'Cauliflower_Bacterial_spot_rot': ('Cauliflower', 'Bacterial_spot_rot'),\n",
    "    'Cauliflower_Black_Rot': ('Cauliflower', 'Black_Rot'),\n",
    "    'Cauliflower_Downy_Mildew': ('Cauliflower', 'Downy_Mildew'),\n",
    "    'Cauliflower_Healthy': ('Cauliflower', 'healthy'),\n",
    "    'EggPlant_Healthy_Leaf': ('Eggplant', 'healthy'),\n",
    "    'EggPlant_Insect_Pest_Disease': ('Eggplant', 'Insect_Pest_Disease'),\n",
    "    'EggPlant_Leaf_Spot_Disease': ('Eggplant', 'Leaf_Spot_Disease'),\n",
    "    'EggPlant_Mosaic_Virus_Disease': ('Eggplant', 'Mosaic_Virus_Disease'),\n",
    "    'EggPlant_Small_Leaf_Disease': ('Eggplant', 'Small_Leaf_Disease'),\n",
    "    'EggPlant_White_Mold_Disease': ('Eggplant', 'White_Mold_Disease'),\n",
    "    'EggPlant_Wilt_Disease': ('Eggplant', 'Wilt_Disease'),\n",
    "\n",
    "    # --- From 'mult_classes/train' (PlantVillage subset) ---\n",
    "    'Apple___Apple_scab': ('Apple', 'Apple_scab'),\n",
    "    'Apple___Black_rot': ('Apple', 'Black_rot'),\n",
    "    'Apple___Cedar_apple_rust': ('Apple', 'Cedar_apple_rust'),\n",
    "    'Apple___healthy': ('Apple', 'healthy'),\n",
    "    'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot': ('Maize', 'Cercospora_leaf_spot_Gray_leaf_spot'),\n",
    "    'Corn_(maize)___Common_rust_': ('Maize', 'Common_rust'),\n",
    "    'Corn_(maize)___healthy': ('Maize', 'healthy'),\n",
    "    'Corn_(maize)___Northern_Leaf_Blight': ('Maize', 'Northern_Leaf_Blight'),\n",
    "    'Grape___Black_rot': ('Grape', 'Black_rot'),\n",
    "    'Grape___Esca_(Black_Measles)': ('Grape', 'Esca_Black_Measles'),\n",
    "    'Grape___healthy': ('Grape', 'healthy'),\n",
    "    'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)': ('Grape', 'Leaf_blight_Isariopsis_Leaf_Spot'),\n",
    "    'Orange___Haunglongbing_(Citrus_greening)': ('Orange', 'Citrus_greening'),\n",
    "    'Peach___Bacterial_spot': ('Peach', 'Bacterial_spot'),\n",
    "    'Peach___healthy': ('Peach', 'healthy'),\n",
    "    'Potato___Early_blight': ('Potato', 'Early_blight'),\n",
    "    'Potato___healthy': ('Potato', 'healthy'),\n",
    "    'Potato___Late_blight': ('Potato', 'Late_blight'),\n",
    "    'Tomato___Bacterial_spot': ('Tomato', 'Bacterial_spot'),\n",
    "    'Tomato___Early_blight': ('Tomato', 'Early_blight'),\n",
    "    'Tomato___healthy': ('Tomato', 'healthy'),\n",
    "    'Tomato___Late_blight': ('Tomato', 'Late_blight'),\n",
    "    'Tomato___Leaf_Mold': ('Tomato', 'Leaf_Mold'),\n",
    "    'Tomato___Septoria_leaf_spot': ('Tomato', 'Septoria_leaf_spot'),\n",
    "    'Tomato___Spider_mites Two-spotted_spider_mite': ('Tomato', 'Spider_mites'),\n",
    "    'Tomato___Target_Spot': ('Tomato', 'Target_Spot'),\n",
    "    'Tomato___Tomato_mosaic_virus': ('Tomato', 'Mosaic_virus'),\n",
    "    'Tomato___Tomato_Yellow_Leaf_Curl_Virus': ('Tomato', 'Yellow_Leaf_Curl_Virus'),\n",
    "\n",
    "    # --- From 'olive/train' (Olive) ---\n",
    "    'aculus_olearius': ('Olive', 'Aculus_olearius_mite'),\n",
    "    'Healthy': ('Olive', 'healthy'),\n",
    "    'olive_peacock_spot': ('Olive', 'Peacock_spot'),\n",
    "}\n",
    "\n",
    "# --- Image Gathering and Renaming Logic (No major change here, assumes it runs once) ---\n",
    "print('--- Standardizing, Renaming, and Gathering Images ---')\n",
    "IMG_OUT.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Using Data Directory: {DATA_DIR}\")\n",
    "\n",
    "METADATA = []\n",
    "\n",
    "COMMON_ROOTS = [\n",
    "    DATA_DIR / 'archive' / 'train',\n",
    "    DATA_DIR / 'Cauliflower' / 'train',\n",
    "    DATA_DIR / 'mult_classes' / 'train',\n",
    "    DATA_DIR / 'olive' / 'train',\n",
    "    \n",
    "    # Also check test/valid folders\n",
    "    DATA_DIR / 'archive' / 'test',\n",
    "    DATA_DIR / 'Cauliflower' / 'test',\n",
    "    DATA_DIR / 'mult_classes' / 'test',\n",
    "    DATA_DIR / 'olive' / 'test',\n",
    "    \n",
    "    DATA_DIR / 'archive' / 'valid',\n",
    "    DATA_DIR / 'Cauliflower' / 'valid',\n",
    "    DATA_DIR / 'mult_classes' / 'valid',\n",
    "    DATA_DIR / 'olive' / 'valid',\n",
    "]\n",
    "\n",
    "# Check if the primary root exists, to be sure\n",
    "if not (DATA_DIR / 'archive' / 'train').is_dir():\n",
    "    print(f\"FATAL ERROR: Could not find {DATA_DIR / 'archive' / 'train'}\")\n",
    "    print(\"Please verify the hardcoded 'ROOT' path in the script.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "for src_folder_name, (crop, disease) in tqdm(CLASS_MAPPING.items(), desc=\"Processing Classes\"):\n",
    "    src = None\n",
    "    \n",
    "    for root in COMMON_ROOTS:\n",
    "        potential_src = root / src_folder_name\n",
    "        if potential_src.is_dir():\n",
    "            src = potential_src\n",
    "            break \n",
    "\n",
    "    if src is None:\n",
    "        continue \n",
    "    \n",
    "    # Destination folder is still the unified location for all images initially:\n",
    "    dst_folder = IMG_OUT / crop / disease \n",
    "    \n",
    "    for root_dir, dirs, files in os.walk(src):\n",
    "        for f in files:\n",
    "            if f.lower().endswith(('.jpg','.jpeg','.png')):\n",
    "                src_file = Path(root_dir) / f\n",
    "                \n",
    "                try:\n",
    "                    new_name = file_hash_name(src_file)\n",
    "                    if new_name is None:\n",
    "                        continue \n",
    "                        \n",
    "                    dst_file = dst_folder / new_name\n",
    "                    \n",
    "                    if dst_file.exists():\n",
    "                        continue \n",
    "                        \n",
    "                    normalize_and_save(src_file, dst_file)\n",
    "                    \n",
    "                    METADATA.append({\n",
    "                        'image_path': str(dst_file.relative_to(OUT_DIR)),\n",
    "                        'crop': crop,\n",
    "                        'disease': disease,\n",
    "                        'source_folder': src_folder_name,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {src_file}: {e}\")\n",
    "                \n",
    "# Save collected metadata\n",
    "meta = pd.DataFrame(METADATA)\n",
    "if len(meta) > 0:\n",
    "    meta.to_csv(METADATA_CSV, index=False)\n",
    "else:\n",
    "    print(\"\\n--- ERROR ---\")\n",
    "    print(\"Collected 0 images. This should not happen with the new mapping.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f'\\nCollected {len(meta)} images.')\n",
    "print(f'The unified dataset images are located in: {IMG_OUT}')\n",
    "\n",
    "\n",
    "# --- Train / Val / Test split (MODIFIED SECTION) ---\n",
    "print('\\n--- Splitting Data and Moving Files ---')\n",
    "if len(meta) > 1 and len(meta['crop'].unique()) > 1:\n",
    "    meta['label'] = meta['crop'] + '___' + meta['disease']\n",
    "    \n",
    "    # Filtering logic remains the same (important for stratified split)\n",
    "    counts = meta['label'].value_counts()\n",
    "    single_image_classes = counts[counts == 1].index\n",
    "    \n",
    "    if len(single_image_classes) > 0:\n",
    "        meta = meta[~meta['label'].isin(single_image_classes)]\n",
    "\n",
    "    if len(meta) > 1:\n",
    "        counts = meta['label'].value_counts()\n",
    "        valid_classes = counts[counts >= 2].index\n",
    "        if len(valid_classes) < len(counts):\n",
    "            meta = meta[meta['label'].isin(valid_classes)]\n",
    "\n",
    "    if len(meta) > 1:\n",
    "        # Perform stratified split\n",
    "        train, temp = train_test_split(meta, stratify=meta['label'], test_size=0.3, random_state=RANDOM_SEED)\n",
    "        val, test = train_test_split(temp, stratify=temp['label'], test_size=0.5, random_state=RANDOM_SEED)\n",
    "\n",
    "        for df, name in [(train,'train'), (val,'val'), (test,'test')]:\n",
    "            print(f'\\nProcessing {name} split...')\n",
    "\n",
    "            # 1. Save metadata CSV\n",
    "            if 'label' in df.columns:\n",
    "                 df_csv = df.drop(columns=['label']).copy()\n",
    "            df_csv.to_csv(OUT_DIR / f'metadata_{name}.csv', index=False)\n",
    "            print(f'{name}: {len(df)} images.')\n",
    "\n",
    "            # 2. Physical File Movement\n",
    "            for index, row in tqdm(df.iterrows(), total=len(df), desc=f'Moving {name} files'):\n",
    "                \n",
    "                # row['image_path'] is relative to OUT_DIR, e.g., 'images/Apple/healthy/hash.jpg'\n",
    "                \n",
    "                # The unified image location (Source)\n",
    "                src_file = OUT_DIR / row['image_path']\n",
    "                \n",
    "                # Determine the new destination path structure: images/SPLIT_NAME/Crop/Disease/hash.jpg\n",
    "                # e.g., images/train/Apple/healthy/hash.jpg\n",
    "                \n",
    "                # The path inside the 'images' folder (e.g., 'Apple/healthy/hash.jpg')\n",
    "                relative_path_in_images = Path(row['image_path']).relative_to('images') \n",
    "                \n",
    "                # Destination folder: IMG_OUT / split_name / relative_path_in_images.parent\n",
    "                dst_folder = IMG_OUT / name / relative_path_in_images.parent \n",
    "                \n",
    "                # Destination file path\n",
    "                dst_file = dst_folder / relative_path_in_images.name\n",
    "                \n",
    "                # Create the destination folder (e.g., jordan_dataset/images/train/Apple/healthy)\n",
    "                dst_folder.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                # Move the file from the unified pool into the split folder\n",
    "                try:\n",
    "                    # Only move if the source file exists and is not already in the final destination\n",
    "                    if src_file.exists() and not dst_file.exists(): \n",
    "                        shutil.move(src_file, dst_file)\n",
    "                except Exception as e:\n",
    "                    print(f\"\\nWarning: Failed to move {src_file} to {dst_file}. Error: {e}\")\n",
    "    else:\n",
    "        print(\"Warning: Not enough data left after filtering to perform stratified split.\")\n",
    "else:\n",
    "    print(\"Warning: Not enough data or classes to perform stratified Train/Val/Test split.\")\n",
    "\n",
    "# --- Simple augmentation (Modified to operate on the new split folders) ---\n",
    "print('\\n--- Augmenting Small Classes ---')\n",
    "MIN_SAMPLES = 500\n",
    "augmenter = A.Compose([\n",
    "    A.RandomRotate90(), \n",
    "    A.HorizontalFlip(),\n",
    "    A.Transpose(),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.1, rotate_limit=15, p=0.7),\n",
    "])\n",
    "\n",
    "augmented_count = 0\n",
    "# We now rely on the 'train' split count, as augmentation only happens on training data\n",
    "if 'train' in locals() and 'label' in train.columns:\n",
    "    counts = train['label'].value_counts()\n",
    "    \n",
    "    # Save the training set class counts\n",
    "    print(\"Saving training set class counts...\")\n",
    "    counts.to_frame('n_images').to_csv(OUT_DIR / 'train_class_counts.csv')\n",
    "\n",
    "    for label, n in counts.items():\n",
    "        if n < MIN_SAMPLES:\n",
    "            need = MIN_SAMPLES - n\n",
    "            print(f'Augmenting {label}, need {need} copies.')\n",
    "            crop, disease = label.split('___')\n",
    "            \n",
    "            # --- OPERATE ON THE 'train' SPLIT FOLDER ---\n",
    "            folder = IMG_OUT / 'train' / crop / disease \n",
    "            \n",
    "            images = [p for p in folder.glob('*.jpg') if '_aug_' not in p.name]\n",
    "            if not images:\n",
    "                print(f\"Warning: No source images found to augment for {label}\")\n",
    "                continue\n",
    "            \n",
    "            i = 0\n",
    "            while need > 0:\n",
    "                src = random.choice(images)\n",
    "                try:\n",
    "                    img = Image.open(src).convert('RGB')\n",
    "                    arr = np.array(img)\n",
    "                    aug = augmenter(image=arr)['image']\n",
    "                    \n",
    "                    new_name = src.stem + f'_aug_{i}.jpg'\n",
    "                    outp = folder / new_name\n",
    "                    Image.fromarray(aug).save(outp, format='JPEG', quality=90)\n",
    "                    \n",
    "                    augmented_count += 1\n",
    "                    need -= 1\n",
    "                    i += 1\n",
    "                except Exception as e:\n",
    "                    print(f'Augment failed for {src}: {e}')\n",
    "\n",
    "print('Augmented images created:', augmented_count)\n",
    "\n",
    "print('\\n✨ Dataset Consolidation and Splitting Complete! ✨')\n",
    "print(f\"The data is ready in the standard PyTorch format:\")\n",
    "print(f\"{IMG_OUT}/train/[Crop]/[Disease]/*.jpg\")\n",
    "print(f\"{IMG_OUT}/val/[Crop]/[Disease]/*.jpg\")\n",
    "print(f\"{IMG_OUT}/test/[Crop]/[Disease]/*.jpg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
